---
title: "Lasso & Ridge Comparison"
author: "Joon Bum Yang"
date: '2023-03-20'
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ISLR)
library(boot)
library(glmnet)
```

## Set up
```{r}
# clear memory
rm(list=ls())
# set global variable
n <- 1100
p <- 50
set.seed(0)
# generate X and epsilon
X <- matrix(rnorm(n*p), nrow=n, ncol=p, byrow=TRUE)
epsilon <- rnorm(n)
# generate beta and fit Y
beta <- c(rep(2,5), rep(0,45))
Y <- X%*%beta + epsilon
# Train/Test split
train_index <-sample(c(1:nrow(Y)),100)
train_y <- Y[train_index]
train_x <- X[train_index,]
test_y <- Y[-train_index]
test_x <- X[-train_index,]
# set grid of lambda
grid = 10^seq(10,-2,length = 100)

```

## Fitting both the ridge regression and the lasso with selected by cross validation on the grid and Compare

```{r}
# ridge regression
cv.ridge <- cv.glmnet(train_x,train_y,alpha=0,lambda=grid)
bestlam <- cv.ridge$lambda.min
ridge.mod <- glmnet(train_x,train_y,alpha=0,lambda=bestlam)
pred.ridge <- predict(ridge.mod,test_x, lambda = bestlam)

ridge.mse <- mean((test_y - pred.ridge)^2)
paste('ridge regression MSE is ', ridge.mse)

# lasso regression
cv.lasso <- cv.glmnet(train_x,train_y,alpha=1,lambda=grid)
bestlam <- cv.lasso$lambda.min
lasso.mod <- glmnet(train_x,train_y,alpha=1,lambda=bestlam)
pred.lasso <- predict(lasso.mod,test_x, lambda = bestlam)
lasso.mse <- mean((test_y - pred.lasso)^2)
paste('lasso regression MSE is ', lasso.mse)
```
We see that Lasso produces smaller test MSE. (Lasso also works as variable selector)

Repeat for different seed numbers to obtain 50 different test MSE results. 
```{r}
mse_gen <- function(seed_num){
  #set up
set.seed(seed_num)
X <- matrix(rnorm(n*p), nrow=n, ncol=p, byrow=TRUE)
epsilon <- rnorm(n)
beta <- c(rep(2,5), rep(0,45))
Y <- X%*%beta + epsilon
train_index <-sample(c(1:nrow(Y)),100)
train_y <- Y[train_index]
train_x <- X[train_index,]
test_y <- Y[-train_index]
test_x <- X[-train_index,]
grid = 10^seq(10,-2,length = 100)

#ridge regression
cv.ridge <- cv.glmnet(train_x,train_y,alpha=0,lambda=grid)
bestlam <- cv.ridge$lambda.min
ridge.mod <- glmnet(train_x,train_y,alpha=0,lambda=bestlam)
pred.ridge <- predict(ridge.mod,test_x, lambda = bestlam)
ridge.mse <- mean((test_y - pred.ridge)^2)
# lasso regression
cv.lasso <- cv.glmnet(train_x,train_y,alpha=1,lambda=grid)
bestlam <- cv.lasso$lambda.min
lasso.mod <- glmnet(train_x,train_y,alpha=1,lambda=bestlam)
pred.lasso <- predict(lasso.mod,test_x, lambda = bestlam)
lasso.mse <- mean((test_y - pred.lasso)^2)
return(c(ridge.mse, lasso.mse))
}

# Store 50 test MSE for both lasso and ridge
MSE_ridge <- c()
MSE_lasso <- c()
for (i in 2:50){
m <- mse_gen(i)
MSE_ridge <- c(MSE_ridge,m[1])
MSE_lasso <- c(MSE_lasso,m[2])
}

#Box_plot to compare
MSE_ridge <- c(ridge.mse,MSE_ridge)
MSE_lasso <- c(lasso.mse,MSE_lasso)
boxplot(MSE_ridge,MSE_lasso,
names = c('ridge','lasso'),
main = 'box plot of ridge and lasso mse',
y_lab = 'mse')
```
On average, the lasso has smaller test MSE than the ridge. The variance of test MSEs of the lasso is also smaller than that of the ridge regression. In this case, the Lasso visibly outperforms the ridge.
This is expected as the true model is sparse, that is, the true coefficients have zeroes.



We can now implement the same method with non-spare true model. 

```{r}
mse_gen_new <- function(seed_num){
set.seed(seed_num)
X <- matrix(rnorm(n*p), nrow=n, ncol=p, byrow=TRUE)
epsilon <- rnorm(n)
beta <- rep(0.5,50)
Y <- X%*%beta + epsilon
train_index <-sample(c(1:nrow(Y)),100)
train_y <- Y[train_index]
train_x <- X[train_index,]
test_y <- Y[-train_index]
test_x <- X[-train_index,]
grid = 10^seq(10,-2,length = 100)
#ridge regression
cv.ridge <- cv.glmnet(train_x,train_y,alpha=0,lambda=grid)
bestlam <- cv.ridge$lambda.min
ridge.mod <- glmnet(train_x,train_y,alpha=0,lambda=bestlam)
pred.ridge <- predict(ridge.mod,test_x, lambda = bestlam)
ridge.mse <- mean((test_y - pred.ridge)^2)
# lasso regression
cv.lasso <- cv.glmnet(train_x,train_y,alpha=1,lambda=grid)
bestlam <- cv.lasso$lambda.min
lasso.mod <- glmnet(train_x,train_y,alpha=1,lambda=bestlam)
pred.lasso <- predict(lasso.mod,test_x, lambda = bestlam)
lasso.mse <- mean((test_y - pred.lasso)^2)
return(c(ridge.mse, lasso.mse))
}

# Store 50 test MSE for both lasso and ridge
MSE_ridge_new <- c()
MSE_lasso_new <- c()
for (i in 0:50){
m <- mse_gen_new(i)
MSE_ridge_new <- c(MSE_ridge_new,m[1])
MSE_lasso_new <- c(MSE_lasso_new,m[2])
}
# leave set.seed(1) out
MSE_ridge_new <- MSE_ridge_new[-2]
MSE_lasso_new <- MSE_lasso_new[-2]

#Box_plot to compare
boxplot(MSE_ridge_new,MSE_lasso_new,
names = c('ridge','lasso'),
main = 'box plot of ridge and lasso mse',
y_lab = 'mse')

```
The ridge regression, on average, has slightly smaller test MSEs than the Lasso. But their variances
of test MSEs are nearly the same. The advantage of the ridge over the lasso in this case seems not
substantial.
This is also expected as the true coefficients have some small but non-zero entries, whence the ridge
should perform (slightly) better.









